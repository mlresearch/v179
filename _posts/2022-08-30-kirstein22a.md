---
title: Tensor-Train Kernel Learning for Gaussian Processes
abstract: 'We propose a new kernel learning approach based on efficient low-rank tensor
  compression for Gaussian process (GP) regression. The central idea is to compose
  a low-rank function represented in a hierarchical tensor format with a GP covariance
  function. Compared to similar deep neural network architectures, this approach facilitates
  to learn significantly more expressive features at lower computational costs as
  illustrated in the examples. Additionally, over-fitting is avoided with this compositional
  model by taking advantage of its inherent regularisation properties. Estimates of
  the generalisation error are compared to five baseline models on three synthetic
  and six real-world data sets. The experimental results show that the incorporated
  tensor network enables a highly accurate GP regression with a comparatively low
  number of trainable parameters. The observed performance is clearly superior (usually
  by an order of magnitude in mean squared error) to all examined standard models,
  in particular to deep neural networks with more than 1000 times as many parameters. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kirstein22a
month: 0
tex_title: Tensor-Train Kernel Learning for Gaussian Processes
firstpage: 253
lastpage: 272
page: 253-272
order: 253
cycles: false
bibtex_author: Kirstein, Max and Sommer, David and Eigel, Martin
author:
- given: Max
  family: Kirstein
- given: David
  family: Sommer
- given: Martin
  family: Eigel
date: 2022-08-30
address:
container-title: Proceedings of the Eleventh Symposium on Conformal and Probabilistic
  Prediction with Applications
volume: '179'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 8
  - 30
pdf: https://proceedings.mlr.press/v179/kirstein22a/kirstein22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
